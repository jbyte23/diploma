{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mKsHPIGi8BtM",
        "z_ArcSDa8Mbe",
        "mLWmt3mxvnoT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjTK6cEoMGsj"
      },
      "source": [
        "**Procesiranje podatkov**\n",
        "\n",
        "V tej beležnici preberemo in shranimo podatke iz podatkovne zbirke slovenskih člankov ter jih pred-procesiramo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKsHPIGi8BtM"
      },
      "source": [
        "# Priprava okolja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTidOKlz6ZJn"
      },
      "source": [
        "!pip install classla"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAiiSqbwu1Iy"
      },
      "source": [
        "import zipfile\n",
        "import tarfile\n",
        "import json\n",
        "import os\n",
        "\n",
        "import classla\n",
        "classla.download('sl')\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/')\n",
        "from utils import read_json_file, save_articles, prepare_dataframe, visualize_articles_by_media, read_preprocessed_specific_media, dataframe_info\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import OrderedDict, defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cygqjCLZ8IHa"
      },
      "source": [
        "# Pomožne funkcije"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCfrOH-TrIpX"
      },
      "source": [
        "def save_extracted_articles(articles, dir):\n",
        "  \"\"\"\n",
        "  Save articles to a file.\n",
        "  :param articles: articles to be saved\n",
        "  \"\"\"\n",
        "\n",
        "  for media in articles.keys():\n",
        "    filename = dir + media\n",
        "    with open(filename, 'w', encoding='utf8') as fp:\n",
        "      json.dump(articles[media], fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPdFMcE_PqAZ"
      },
      "source": [
        "def read_data_json(json_file, articles_by_media):\n",
        "  \"\"\"\n",
        "  This function reads a single json file and it returns a dictionary of articles\n",
        "  in json_file\n",
        "\n",
        "  :param json_file: json file\n",
        "  :param articles_by_media: a dictionary of media names as keys and articles as\n",
        "  values\n",
        "  :return: articles_by_media (new articles added)\n",
        "  \"\"\"\n",
        "\n",
        "  data = json.load(json_file)\n",
        "\n",
        "  articles_full = data['articles']['results']       # a dictionary (JSON) of all articles' metadata\n",
        "\n",
        "  for article in articles_full:\n",
        "    body = article['body']\n",
        "    media = article['source']['title']\n",
        "    title = article['title']\n",
        "    \n",
        "    if media not in articles_by_media.keys():\n",
        "      articles_by_media[media] = {}\n",
        "      articles_by_media[media]['body'] = []\n",
        "      articles_by_media[media]['title'] = []\n",
        "\n",
        "    articles_by_media[media]['body'].append(body)\n",
        "    articles_by_media[media]['title'].append(title)\n",
        "\n",
        "  return articles_by_media  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjdsbvrAp0oz"
      },
      "source": [
        "def read_data_zip(filepath):\n",
        "\n",
        "  \"\"\"\n",
        "  Read and save data from a zip file of dataset of Slovenian articles. A zip file contains 7 tar.gz \n",
        "  files, each one for a year from 2014 and 2020.\n",
        "\n",
        "  :param filepath: path to the data zip file\n",
        "  \"\"\"\n",
        "\n",
        "  with zipfile.ZipFile(filepath, 'r') as zip_file:\n",
        "\n",
        "    year = 2014\n",
        "    for year_file in zip_file.namelist()[1:8]:  \n",
        "      save_path = f'/content/drive/MyDrive/Colab Notebooks/raw_articles/{year}/'\n",
        "\n",
        "      articles_by_media = {}\n",
        "      \n",
        "      zip_file.extract(year_file)\n",
        "      tar = tarfile.open(year_file)\n",
        "\n",
        "      for member in tar.getmembers()[1:]:\n",
        "        json_file = tar.extractfile(member.name)\n",
        "        articles_by_media = read_data_json(json_file, articles_by_media)\n",
        "\n",
        "      try:\n",
        "        save_extracted_articles(articles_by_media, save_path)\n",
        "      except FileNotFoundError as err:\n",
        "        print(err)\n",
        "      \n",
        "      year += 1  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcyF5nCgxSyK"
      },
      "source": [
        "def preprocess_articles(articles, stop_words, nlp):\n",
        "  \"\"\"\n",
        "  Preprocess a list of raw articles. Remove words in stop_words list and are \n",
        "  shorter than 4 words from each article from article list and lemmatize each \n",
        "  word with nlp pipeline.\n",
        "\n",
        "  :param articles: list of strings to preprocess\n",
        "  :param stop_words: list of words to be removed from articles\n",
        "  :param nlp: stanza pipeline for word lemmatization\n",
        "\n",
        "  :return preprocessed_articles: a list of preprocessed articles (lists of lemmas)\n",
        "  \"\"\"\n",
        "\n",
        "  preprocessed_articles = []    # list of preprocessed articles\n",
        "\n",
        "  for article in articles:\n",
        "    \n",
        "\n",
        "    doc = nlp(article)\n",
        "\n",
        "    preprocessed = []\n",
        "    for word in doc.iter_tokens():\n",
        "      word_dict = word.to_dict()[0]\n",
        "\n",
        "      if word_dict['upos'] in ['NOUN', 'ADJ', 'PROPN', 'VERB']:\n",
        "        preprocessed.append(word_dict['lemma'])\n",
        "\n",
        "\n",
        "      preprocessed_body = []     # a list of words of a single article\n",
        "      for token in simple_preprocess(article, min_len=4, max_len=25):\n",
        "        # remove all words shorter than three characters\n",
        "        if token not in stop_words:\n",
        "          preprocessed_body.append(token)\n",
        "\n",
        "      doc = nlp(' '.join(preprocessed_body))\n",
        "      lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "\n",
        "      preprocessed_articles.append(lemmas)\n",
        "\n",
        "  return preprocessed_articles\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz4NwQYRsPub"
      },
      "source": [
        "def preprocess_media_articles(media_list, load_dir, save_dir):\n",
        "  \"\"\"\n",
        "  Preprocess articles from media_list files in load_dir and save them to save_dir\n",
        "\n",
        "  :param media_list: a list of media names we want to preprocess\n",
        "  :param load_dir: a path to directory of files with raw articles\n",
        "  :param save_dir: a path to directory where preprocessed files will be saved\n",
        "  \"\"\"\n",
        "\n",
        "  stop_words = stopwords.words('slovene')\n",
        "  new_sw = [\"href\", \"http\", \"https\", \"quot\", \"nbsp\", \"mailto\", \"mail\", \"getty\", \"foto\", \"images\", \"urbanec\", \"sportid\"]\n",
        "  stop_words.extend(new_sw)\n",
        "\n",
        "  filepath = '/content/drive/MyDrive/Colab Notebooks/stopwords'\n",
        "  with open(filepath, 'r') as f:\n",
        "    additional_stopwords = f.read().splitlines()\n",
        "\n",
        "  stop_words.extend(additional_stopwords)\n",
        "  stop_words = list(set(stop_words))\n",
        "\n",
        "  config = {\n",
        "  \t'processors': 'tokenize, lemma', # Comma-separated list of processors to use\n",
        "  \t'lang': 'sl', # Language code for the language to build the Pipeline in\n",
        "    'tokenize_pretokenized': True, # Use pretokenized text as input and disable tokenization\n",
        "    'use_gpu': True\n",
        "  }\n",
        "  nlp = classla.Pipeline(**config)\n",
        "\n",
        "  \n",
        "  for file in os.listdir(load_dir):\n",
        "\n",
        "    if file not in media_list:\n",
        "      continue\n",
        "\n",
        "    save_filepath = save_dir + file\n",
        "    if os.path.exists(save_filepath):\n",
        "      print(\"File \", file, \" already exists\")\n",
        "      continue\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.mkdir(save_dir)\n",
        "\n",
        "    load_filepath = load_dir + file\n",
        "    articles = read_json_file(load_filepath)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(articles)\n",
        "    df['word_length'] = df.body.apply(lambda x: len(str(x).split()))\n",
        "    df = df.loc[df['word_length'] > 25]\n",
        "    df = df.drop_duplicates(subset='title', keep=\"last\")\n",
        "    df = df.drop('word_length', axis=1)\n",
        "    articles = df.to_dict('list')\n",
        "\n",
        "    \n",
        "    print(f\"Preprocessing file: {file} with {len(articles['body'])} articles\")\n",
        "    preprocessed_articles = preprocess_articles(articles['body'], stop_words, nlp)\n",
        "    \n",
        "    save_articles(preprocessed_articles, save_filepath)\n",
        "    print(f\"File saved to {save_filepath}!\\n**********************\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_ArcSDa8Mbe"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSCKE_ILjRmo"
      },
      "source": [
        "Nastavljanje konstant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ruoWZ7W2SE2"
      },
      "source": [
        "YEAR = 2020\n",
        "media_list = ['Dnevnik', 'MMC RTV Slovenija', '24ur.com', 'Siol.net Novice', 'Nova24TV', 'Tednik Demokracija', 'PortalPolitikis']\n",
        "load_dir = f'/content/drive/MyDrive/Colab Notebooks/raw_articles/{YEAR}/'\n",
        "save_dir = f'/content/drive/MyDrive/Colab Notebooks/preprocessed_articles/{YEAR}/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLWmt3mxvnoT"
      },
      "source": [
        "## **Predprocesiranje člankov**\n",
        "\n",
        "V tem delu se prebere članke navedenih medijev v media_list, odstrani tiste s krajšim besedilom od 25 besed in tiste, ki imajo znotraj posameznega medija enake naslove (duplikati).\n",
        "\n",
        "Nato vsak članek razdeli na besede (angl. tokenize), odstrani vse besede, ki so v stop_words (besede, ki nimajo nekega pomena, npr. da, tako, in...) in ki so krajše od 4 črk. Besede, ki so ostale lematiziramo (spremenimo v osnovno obliko)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlCAbU6swMH_"
      },
      "source": [
        "preprocess_media_articles(media_list, load_dir, save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdq1ZbncN5nj"
      },
      "source": [
        "**Post-procesiranje**\n",
        "\n",
        "Pri določenih medijih se določeni deli člankov pojavljajo v mnogih člankih, zato je smiselno te ponavljajoče se dele odstraniti vsaj iz že pred-procesiranih člankov.\n",
        "\n",
        "*Slovenska tiskovna agencija STA:* Vsak članek se začne na način: 'Ljubljana, 29. oktobra (STA)' - 'vsebina članka'. Te dele torej odstranimo.\n",
        "\n",
        "*24ur.com*: Veliko člankov ima na začetku članka del besedila, ki se nanaša na omogočanje piškotkov spletnega mesta. Ta del odstranimo iz člankov.\n",
        "\n",
        "*Siol.net Novice*: Veliko člankov se začne z besedilom, ki se nanaša ta t.i. *termometer*, ki bralcu razloži vlogo le-tega pri poročanju o popularnosti članka. Tudi te dele odstranimo iz člankov.\n",
        "\n",
        "\n",
        "Poleg tega odstranimo tudi članke z manj kot 25 besedami."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2RnnnApPWjc"
      },
      "source": [
        "df = prepare_dataframe(media_list, YEAR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx-_pkqVN4HH"
      },
      "source": [
        "# ZA STA PREDPROCESIRANE BODY-JE\n",
        "\n",
        "print(df.loc[df.media == 'Slovenska tiskovna agencija STA', 'preprocessed_body'])\n",
        "# df.loc[df.media == 'Slovenska tiskovna agencija STA', 'preprocessed_body'] = df.loc[df.media == 'Slovenska tiskovna agencija STA', 'preprocessed_body'].apply(lambda x: x[1:])\n",
        "\n",
        "# Če izhod naslednje vrstice na začetku vsakega seznama ne vsebuje več imena kraja ali meseca, smo odstranili ponavljajoče se dele\n",
        "print(df.loc[df.media == 'Slovenska tiskovna agencija STA', 'preprocessed_body'])\n",
        "\n",
        "# # ZA Siol PREDPROCESIRANE BODY-JE\n",
        "# df.loc[df.media == 'Siol.net Novice', 'preprocessed_body'] = df.loc[df.media == 'Siol.net Novice', 'preprocessed_body'].apply(lambda x: x[10:] if x[0] == 'termometer' else x)\n",
        "\n",
        "# Če je izhod naslednje vrstice enak '['ne']', potem smo odstranili ponavljajoče se dele besedila\n",
        "print(df.loc[df.media == 'Siol.net Novice', 'preprocessed_body'].apply(lambda x: 'ja' if x[0] == 'termometer' else 'ne').unique())\n",
        "\n",
        "# # ZA 24ur.com PREDPROCESIRANE BODY-JE\n",
        "# df.loc[df.media == '24ur.com', 'preprocessed_body'] = df.loc[df.media == '24ur.com', 'preprocessed_body'].apply(lambda x: x[10:] if 'piškotek' in x[:9] else x)\n",
        "\n",
        "# Če je izhod naslednje vrstice enak '['ne']', potem smo odstranili ponavljajoče se dele besedila\n",
        "print(df.loc[df.media == '24ur.com', 'preprocessed_body'].apply(lambda x: 'ja' if 'piškotek' in x[:9] else 'ne').unique())\n",
        "\n",
        "\n",
        "# save_preprocessed_articles(df.loc[df.media == 'Slovenska tiskovna agencija STA', 'preprocessed_body'].to_list(), '/content/gdrive/MyDrive/Colab Notebooks/preprocessed_articles/'+ str(2017) + '/' + 'Slovenska tiskovna agencija STA')\n",
        "# save_preprocessed_articles(df.loc[df.media == 'Siol.net Novice', 'preprocessed_body'].to_list(), '/content/gdrive/MyDrive/Colab Notebooks/preprocessed_articles/'+ str(YEAR) + '/' + 'Siol.net Novice')\n",
        "# save_preprocessed_articles(df.loc[df.media == '24ur.com', 'preprocessed_body'].to_list(), '/content/gdrive/MyDrive/Colab Notebooks/preprocessed_articles/'+ str(YEAR) + '/' + '24ur.com')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ODXM_oxum9"
      },
      "source": [
        "## **Predstavitev končnih podatkov**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqxRNZfk8ntL"
      },
      "source": [
        "Prikaz števila člankov posameznega leta v določenem letu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBrKh2ts5hdq"
      },
      "source": [
        "count = {}\n",
        "for f in os.listdir(load_dir):\n",
        "  if os.path.isfile(f'{load_dir}{f}'):\n",
        "    articles = read_json_file(f'{load_dir}{f}')\n",
        "    count[f] = len(articles['body'])\n",
        "\n",
        "count = dict(sorted(count.items(), key=lambda item: item[1], reverse=True)[:20])\n",
        "\n",
        "visualize_articles_by_media(list(count.keys()), list(count.values()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mBiQN_c3LsQ"
      },
      "source": [
        "df = prepare_dataframe(media_list, YEAR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnJRvEfP8xEC"
      },
      "source": [
        "Prikaz števila člankov izbranih medijev v izbranem letu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blrgv-eD3NmW"
      },
      "source": [
        "count_articles = df.media.value_counts().to_dict()\n",
        "media_names = list(count_articles.keys())\n",
        "counts = list(count_articles.values())\n",
        "print(f'Število vseh člankov skupaj: {sum(counts)}')\n",
        "visualize_articles_by_media(count_articles, counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK-bP5fk86_E"
      },
      "source": [
        "Prikaz števila besed v člankih izbranih medijev (skupno)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orsd4Pag86oY"
      },
      "source": [
        "dataframe_info(df, 'word_length')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChkLjUg69WLu"
      },
      "source": [
        "Prikaz števila besed v člankih izbranih medijev (vsak medij posebej)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiqN6_Dr4XqV"
      },
      "source": [
        "for media in media_list:\n",
        "  print(f'\\n{media}')\n",
        "  dataframe_info(df.loc[df.media == media], 'word_length', media)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}