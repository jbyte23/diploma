{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment_train.ipynb","provenance":[],"collapsed_sections":["Iwu5j0t8nnOx","l-qjp5cRocRp"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Iwu5j0t8nnOx"},"source":["# Priprava okolja"]},{"cell_type":"code","metadata":{"id":"ZEIcSEDqf1Xj"},"source":["!pip install transformers\n","!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_lUa8M5g9G9"},"source":["import csv\n","import torch\n","from torch.utils.data import WeightedRandomSampler\n","import torch.nn.functional as F\n","from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, AdamW\n","import pandas as pd\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","\n","import transformers\n","import json\n","import numpy as np\n","import seaborn as sns\n","from tqdm import tqdm\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n","from collections import defaultdict\n","from textwrap import wrap\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","\n","RANDOM_SEED = 42\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7aZ8cB7qX9iI"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l-qjp5cRocRp"},"source":["# Pomožne funkcije in razredi"]},{"cell_type":"markdown","metadata":{"id":"7Wwxfo3mnwan"},"source":["Branje podatkov"]},{"cell_type":"code","metadata":{"id":"cRZqXGRWwSvV"},"source":["def read_data(file):\n","  senti_text = []\n","  with open(file, 'r', encoding=\"utf8\") as f:\n","      reader = csv.reader(f, delimiter=\"\\t\")\n","      i = 0\n","      for line in reader:\n","          if i == 0:\n","              i = 1\n","              continue\n","          sentiment = line[-1]\n","          text = line[5]\n","          senti_text.append((text, sentiment))\n","          i += 1\n","\n","  return senti_text\n","\n","\n","def sentiment_to_int(label):\n","  if label == \"negative\":\n","    return 0\n","  elif label == \"neutral\":\n","    return 1\n","  elif label == \"positive\":\n","    return 2\n","  else: \n","    return label\n","\n","def int_to_sentiment(label):\n","  if label == 0:\n","    return \"negative\"\n","  elif label == 1:\n","    return \"neutral\"\n","  elif label == 2:\n","    return \"positive\"\n","  else: \n","    return label\n","\n","\n","def prepare_data(filepath):\n","  data = read_data(filepath)\n","\n","  data_df = pd.DataFrame.from_records(data)\n","  data_df.columns = ['text', 'sentiment']\n","  \n","  data_df.sentiment = data_df.loc[:, 'sentiment'].apply(sentiment_to_int)\n","\n","  return data_df\n","\n","\n","def word_count_info(df):\n","  print(df.describe())\n","  # articles_word_limit = articles_nonNull[articles_nonNull[‘word_length’] > 60]\n","  plt.figure(figsize=(12,6)) \n","  p1=sns.kdeplot(df['Word count'], shade=True, color='r')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ViUYrLUIowXN"},"source":["Razdelitev podatkov v učno, validacijsko in testno množico"]},{"cell_type":"code","metadata":{"id":"PbQVndcoogfi"},"source":["def split_to_train_test(df, max_len=512, train_size=0.8):\n","\n","  df_train, df_test = train_test_split(\n","    df,\n","    test_size=1 - train_size,\n","    random_state=RANDOM_SEED\n","  )\n","  df_val, df_test = train_test_split(\n","    df_test,\n","    test_size=0.5,\n","    random_state=RANDOM_SEED\n","  )\n","\n","  df_train = df_train.reset_index(drop=True)\n","  df_test = df_test.reset_index(drop=True)\n","  df_val = df_val.reset_index(drop=True)\n","\n","  print(df_train.sentiment.value_counts())\n","  print(df_val.sentiment.value_counts())\n","  print(df_test.sentiment.value_counts())\n","\n","  print(\"FULL Dataset: {}\".format(df.shape))\n","  print(\"TRAIN Dataset: {}\".format(df_train.shape))\n","  print(\"VALIDATION Dataset: {}\".format(df_val.shape))\n","  print(\"TEST Dataset: {}\".format(df_test.shape))\n","\n","  training_set = ArticleDataset(df_train, tokenizer, max_len)\n","  validation_set = ArticleDataset(df_train, tokenizer, max_len)\n","  testing_set = ArticleDataset(df_test, tokenizer, max_len)\n","\n","  return training_set, validation_set, testing_set"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9O-k-euo-xe"},"source":["Razred ArticleDataset"]},{"cell_type":"code","metadata":{"id":"phW3zTb1pYgU"},"source":["class ArticleDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.df = dataframe\n","        self.text = dataframe.text\n","        self.sentiment = dataframe.sentiment\n","        self.max_len = max_len\n","\n","    def __getitem__(self, idx):\n","        text = str(self.text[idx])\n","\n","        inputs = tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=self.max_len,\n","            return_attention_mask=True,\n","            return_token_type_ids=True\n","        )\n","      \n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","        # token_type_ids = inputs[\"token_type_ids\"]\n","\n","        return {\n","            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n","            # 'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.sentiment[idx], dtype=torch.float)\n","        }\n","\n","    def __len__(self):\n","        return len(self.text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VT5Rim_JpZcf"},"source":["Razred SentimentClassifier"]},{"cell_type":"code","metadata":{"id":"UuqUqUULpe8k"},"source":["class SentimentClassifier(nn.Module):\n","\n","  def __init__(self, n_classes):\n","    super(SentimentClassifier, self).__init__()\n","    self.model = AutoModel.from_pretrained('EMBEDDIA/sloberta')\n","    self.pre_classifier = torch.nn.Linear(768, 768)\n","    self.dropout = torch.nn.Dropout(0.2)\n","    self.classifier = nn.Linear(self.model.config.hidden_size, n_classes)        \n","\n","  def forward(self, input_ids, attention_mask):\n","    output = self.model(\n","        input_ids=input_ids, \n","        attention_mask=attention_mask\n","        )\n","    last_hidden_state = output[0]\n","    pooler = last_hidden_state[:, 0, :]\n","    pooler = self.dropout(pooler)\n","    pooler = self.pre_classifier(pooler)\n","    pooler = torch.nn.ReLU()(pooler)\n","    pooler = self.dropout(pooler)\n","    output = self.classifier(pooler)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8xaqRstppic"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"LBCAB6syptvv"},"source":["Branje in predstavitev podatkov"]},{"cell_type":"code","metadata":{"id":"K5XHwE8On03C"},"source":["# Spremenite pot do datoteke z anotiranimi članki.\n","filepath = '/content/drive/MyDrive/Colab Notebooks/SentiNews_document-level.txt'\n","df = prepare_data(filepath)\n","df['Word count'] = df.text.apply(lambda x: len(str(x).split()))\n","\n","# print(df['sentiment'].value_counts().sort_index())\n","sentiment = ['Negative', 'Neutral', 'Positive']\n","counts = df['sentiment'].value_counts().sort_index()\n","color = ['tomato', 'lightgrey', 'limegreen']\n","\n","fig, ax = plt.subplots()\n","ax.bar(np.arange(len(sentiment)), counts, color=color)\n","ax.set_xticks(np.arange(len(sentiment)))\n","ax.set_xticklabels(sentiment)\n","\n","for i, v in enumerate(counts):\n","  ax.text(i - .1, v + 5, str(v))\n","\n","word_count_info(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CArTkklmsDla"},"source":["Nalaganje tokenizerja"]},{"cell_type":"code","metadata":{"id":"lzZkq-oIsFXV"},"source":["tokenizer = AutoTokenizer.from_pretrained('EMBEDDIA/sloberta', use_fast=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLEjGdZbp0Df"},"source":["Priprava učnih, validacijskih in testnih podatkov"]},{"cell_type":"code","metadata":{"id":"YLy2mNcEJepS"},"source":["MAX_LEN = 512\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 8\n","\n","train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","val_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': False,\n","                'num_workers': 0\n","                }\n","\n","training_set, validation_set, testing_set = split_to_train_test(df, max_len=MAX_LEN, train_size=.8)\n","\n","train_dataloader = DataLoader(training_set, **train_params)\n","val_dataloader = DataLoader(validation_set, **test_params)\n","test_dataloader = DataLoader(testing_set, **test_params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWH7MShvqIJJ"},"source":["Učenje modela"]},{"cell_type":"code","metadata":{"id":"X3uM2zi5mt8Q"},"source":["def train_epoch(model, train_dataloader, optimizer, scheduler, loss_fn_, n_examples):\n","\n","  model = model.train()\n","\n","  losses = []\n","  correct_predictions = 0\n","\n","  for step, d in enumerate(train_dataloader):\n","\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    targets = d[\"targets\"].to(device, dtype = torch.long)\n","\n","    outputs = model(input_ids, attention_mask)\n","\n","    _, preds = torch.max(outputs, dim=1)\n","    correct_predictions += torch.sum(preds == targets)\n","\n","    loss = loss_fn(outputs, targets)\n","    losses.append(loss.item())\n","\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()    \n","\n","    if step % 100 == 0 and step != 0:\n","      loss_step = np.mean(losses)\n","      accu_step = (correct_predictions*100)/(step * targets.size(0)) \n","      print(f\"Training Loss per 2000 steps: {loss_step}\")\n","      print(f\"Training Accuracy per 2000 steps: {accu_step}\")\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","\n","\n","\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","\n","  model = model.eval()\n","  losses = []\n","  correct_predictions = 0\n","  predictions = []\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device, dtype = torch.long)\n","\n","      outputs = model(input_ids, attention_mask)\n","\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, targets)\n","  \n","      correct_predictions += torch.sum(preds == targets)\n","      predictions.extend(preds)\n","\n","      losses.append(loss.item())\n","\n","  predictions = torch.stack(predictions).cpu()\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses), predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pp0Z-WESsQl1"},"source":["Inicializacija modela"]},{"cell_type":"code","metadata":{"id":"XvELQycCsPt9"},"source":["num_of_labels = 3\n","model = SentimentClassifier(num_of_labels)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pGKIXfBhsSti"},"source":["Učenje"]},{"cell_type":"code","metadata":{"id":"1cBcKTf7VJ0V"},"source":["EPOCHS = 4\n","\n","# Po želji spremenite mapo za shranjevanje modela\n","save_dir = '/content/drive/MyDrive/Diploma/best_model_state_latest.bin'\n","\n","optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=True)\n","\n","total_steps = len(train_dataloader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)\n","\n","best_accuracy = 0\n","\n","for epoch in range(0, EPOCHS):\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","\n","  train_acc, train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, loss_fn, len(training_set))\n","  print(f'Train loss {train_loss}\\n Train accuracy {train_acc}')\n","\n","  val_acc, val_loss, preds = eval_model(model, val_dataloader, loss_fn, device, len(validation_set))\n","  print(f'Validation   loss {val_loss}\\n Validation accuracy {val_acc}')\n","  print()\n","\n","  if val_acc > best_accuracy:\n","    torch.save(model.state_dict(), save_dir)\n","    best_accuracy = val_acc\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TrUphQbSrPR5"},"source":["Testiranje modela"]},{"cell_type":"code","metadata":{"id":"KFlZjG37nRym"},"source":["test_acc, test_loss, preds = eval_model(model, test_dataloader, loss_fn, device, len(testing_set))\n","print(f'Test   loss {test_loss}\\n Test accuracy {test_acc}')\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dnXu4x0tCQON"},"source":["class_names = ['negative', 'neutral', 'positive']\n","print(classification_report(testing_set.sentiment, preds, target_names=class_names))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIY41WB6Eua2"},"source":["sns.heatmap(confusion_matrix(testing_set.sentiment, preds)/np.sum(confusion_matrix(testing_set.sentiment, preds)), annot=True, fmt='.2%', cmap='Blues')"],"execution_count":null,"outputs":[]}]}